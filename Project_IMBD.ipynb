{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42049396",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDb Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494690e",
   "metadata": {},
   "source": [
    "## Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77df21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7f2fc",
   "metadata": {},
   "source": [
    "## Loading the Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ecc3e7",
   "metadata": {},
   "source": [
    "* In this stage of loading the data, we have a folder called \"aclImdb\" that contains the IMDB dataset for sentiment analysis. The dataset is divided into two main folders: \"train\" and \"test\". Each of these folders further contains two subfolders, \"positive\" and \"negative\". The \"positive\" folder contains movie reviews with positive sentiment, while the \"negative\" folder contains reviews with negative sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8848eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_train_test_imdb_data(data_dir):\n",
    "\n",
    "    data = {}\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        data[split] = []\n",
    "        for sentiment in [\"neg\", \"pos\"]:\n",
    "            score = 1 if sentiment == \"pos\" else 0\n",
    "\n",
    "            path = os.path.join(data_dir, split, sentiment)\n",
    "            file_names = os.listdir(path)\n",
    "            for f_name in file_names:\n",
    "                with open(os.path.join(path, f_name), \"r\", encoding=\"utf-8\") as f:\n",
    "                    review = f.read()\n",
    "                    data[split].append([review, score])\n",
    "\n",
    "    np.random.shuffle(data[\"train\"])        \n",
    "    data[\"train\"] = pd.DataFrame(data[\"train\"], columns=['text', 'sentiment'])\n",
    "    print(data[\"train\"])\n",
    "    np.random.shuffle(data[\"test\"])\n",
    "    data[\"test\"] = pd.DataFrame(data[\"test\"], columns=['text', 'sentiment'])\n",
    "    print(data[\"test\"])\n",
    "    return data[\"train\"], data[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcdd37b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  sentiment\n",
      "0      Just saw the World Preem of Fido at the Toront...          1\n",
      "1      So we compromised. This was a fairly charming ...          1\n",
      "2      A wonderful surprise of the Spanish cinema. I ...          1\n",
      "3      Psychotic transsexual Bobbi murders the patien...          1\n",
      "4      The producers of this film offer to pay funera...          0\n",
      "...                                                  ...        ...\n",
      "24995  After watching this film last night on Sundanc...          1\n",
      "24996  First things first, the female lead is too gor...          1\n",
      "24997  I don't know what the previous reviewer was wa...          1\n",
      "24998  OK the director remakes LOVE ACTUALLY The dire...          0\n",
      "24999  This comic book style film is funny, has nicel...          1\n",
      "\n",
      "[25000 rows x 2 columns]\n",
      "                                                    text  sentiment\n",
      "0      Like many a child born in the 1980's, I grew u...          1\n",
      "1      The movie is good and I think Tiffany Amber is...          1\n",
      "2      'Never Been Kissed' is a real feel good film. ...          1\n",
      "3      I must have been around ten years old when my ...          1\n",
      "4      This was one of the slowest movies I have ever...          0\n",
      "...                                                  ...        ...\n",
      "24995  This movie is bad as we all knew it would be. ...          0\n",
      "24996  I've just revisited this fondly remembered bit...          1\n",
      "24997  i think south park is hilarious, and i have no...          0\n",
      "24998  I must admit, ashamed though I am, that as an ...          0\n",
      "24999  I was skeptical before going to this because o...          1\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = load_train_test_imdb_data(\n",
    "    data_dir=\"aclImdb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f684bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elia Kazan, one of the best theater directors ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>**Possible Spoilers Ahead**&lt;br /&gt;&lt;br /&gt;\\tJason...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For your own good, it would be best to disrega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well i do disagreed with the other comment pos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes, you guessed it. Another movie where ident...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>being a NI supporter, it's hard to objectively...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>There are bad movies, terrible movies even bor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>With part reconstruction and part direct shoot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>James Bishop (Matt Stasi) goes to a `mental il...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>When I saw this movie in the theater when it c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((NB: Spoiler warning, such as it is!))&lt;br /&gt;&lt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tim (Gary Daniels) wants desperately to break ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>In a time when Hollywood is making money by sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I really can't say too much more about the plo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I like musicals but as a Dickens fan I HATE th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chris Gerolmo took care not to simply give us ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In the hands of lesser actors than Claudette C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>S.S. Van Dine must have been a shrewd business...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>While this was a better movie than 101 Dalmati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Yes, Shakespeare would indeed have been proud....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>When will people learn that some movies are ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>As said before, the visual effects are stunnin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Just like Al Gore shook us up with his painful...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I was totally impressed by Shelley Adrienne's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I was glad to watch this movie free of charge ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Reese Witherspoon plays Dani, a young country ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I would of enjoyed this film but Van Damme jus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The mere fact that I still think of the movie ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I saw this film premiere Friday (1/19) night i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Neatly skipping over everything from the coup ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\"The Last Wave\" is one of those movies that re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>This movie is by far the cutest I have seen in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Although most Americans have little knowledge ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>It is hard to make an unbiased judgment on a f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>\"An album of songs so old everyone thinks they...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>This is exactly the reason why many people rem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I anticipated this movie to be decent and poss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>All good movies \"inspire\" some direct to video...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Well not actually. This movie is very entertai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;I have seen this movie many times....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "0   Elia Kazan, one of the best theater directors ...          1\n",
       "1   **Possible Spoilers Ahead**<br /><br />\\tJason...          0\n",
       "2   For your own good, it would be best to disrega...          0\n",
       "3   Well i do disagreed with the other comment pos...          1\n",
       "4   Yes, you guessed it. Another movie where ident...          0\n",
       "5   being a NI supporter, it's hard to objectively...          0\n",
       "6   There are bad movies, terrible movies even bor...          0\n",
       "7   With part reconstruction and part direct shoot...          1\n",
       "8   James Bishop (Matt Stasi) goes to a `mental il...          0\n",
       "9   When I saw this movie in the theater when it c...          1\n",
       "10  ((NB: Spoiler warning, such as it is!))<br /><...          0\n",
       "11  Tim (Gary Daniels) wants desperately to break ...          0\n",
       "12  In a time when Hollywood is making money by sh...          1\n",
       "13  I really can't say too much more about the plo...          1\n",
       "14  I like musicals but as a Dickens fan I HATE th...          0\n",
       "15  Chris Gerolmo took care not to simply give us ...          1\n",
       "16  In the hands of lesser actors than Claudette C...          1\n",
       "17  S.S. Van Dine must have been a shrewd business...          1\n",
       "18  While this was a better movie than 101 Dalmati...          1\n",
       "19  Yes, Shakespeare would indeed have been proud....          1\n",
       "20  When will people learn that some movies are ma...          1\n",
       "21  As said before, the visual effects are stunnin...          0\n",
       "22  Just like Al Gore shook us up with his painful...          1\n",
       "23  I was totally impressed by Shelley Adrienne's ...          1\n",
       "24  I was glad to watch this movie free of charge ...          0\n",
       "25  Reese Witherspoon plays Dani, a young country ...          1\n",
       "26  I would of enjoyed this film but Van Damme jus...          0\n",
       "27  The mere fact that I still think of the movie ...          1\n",
       "28  I saw this film premiere Friday (1/19) night i...          1\n",
       "29  Neatly skipping over everything from the coup ...          1\n",
       "30  \"The Last Wave\" is one of those movies that re...          1\n",
       "31  This movie is by far the cutest I have seen in...          1\n",
       "32  Although most Americans have little knowledge ...          1\n",
       "33  It is hard to make an unbiased judgment on a f...          0\n",
       "34  \"An album of songs so old everyone thinks they...          1\n",
       "35  This is exactly the reason why many people rem...          0\n",
       "36  I anticipated this movie to be decent and poss...          1\n",
       "37  All good movies \"inspire\" some direct to video...          1\n",
       "38  Well not actually. This movie is very entertai...          1\n",
       "39  <br /><br />I have seen this movie many times....          1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227d748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8146cbc4",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a630034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    \n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b04b7",
   "metadata": {},
   "source": [
    "####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb9fec",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0a443",
   "metadata": {},
   "source": [
    "* To further enhance the model's performance, we apply feature selection using a Genetic Algorithm. This algorithm utilizes evolutionary principles to iteratively select the most informative features from the dataset. We employ the GeneticSelectionCV class, which integrates the Genetic Algorithm with cross-validation for optimal feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a29532e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement feature_selection_genetic (from versions: none)\n",
      "ERROR: No matching distribution found for feature_selection_genetic\n"
     ]
    }
   ],
   "source": [
    "pip install feature_selection_genetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe583913",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feature_selection_genetic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_selection_genetic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeneticSelectionCV\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'feature_selection_genetic'"
     ]
    }
   ],
   "source": [
    "from feature_selection_genetic import GeneticSelectionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471783d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from feature_selection_genetic import GeneticSelectionCV\n",
    "\n",
    "\n",
    "# Transform each text into a vector of word counts\n",
    "vectorizere = CountVectorizer(stop_words=\"english\",\n",
    "                             preprocessor=clean_text\n",
    "                            )\n",
    "\n",
    "\n",
    "X = vectorizere.fit_transform(data[\"text\"])\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "\n",
    "\n",
    "# Feature selection using Genetic Algorithm\n",
    "selector = GeneticSelectionCV(LinearSVC(),\n",
    "                              cv=5,\n",
    "                              verbose=1,\n",
    "                              scoring=\"accuracy\",\n",
    "                              n_population=50,\n",
    "                              crossover_proba=0.5,\n",
    "                              mutation_proba=0.2,\n",
    "                              n_generations=10,\n",
    "                              crossover_independent_proba=0.5,\n",
    "                              mutation_independent_proba=0.05,\n",
    "                              tournament_size=3,\n",
    "                              caching=True,\n",
    "                              n_jobs=-1)\n",
    "selector.fit(X, y)\n",
    "\n",
    "\n",
    "# Select the best features\n",
    "X_selected = selector.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "model = LinearSVC()\n",
    "model.fit(X_selected, y)\n",
    "\n",
    "\n",
    "# Testing\n",
    "test_features = vectorizer.transform(test_data[\"text\"])\n",
    "test_features_selected = selector.transform(test_features)\n",
    "y_pred = model.predict(test_features_selected)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
    "print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa996df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0079b4dd",
   "metadata": {},
   "source": [
    "## SVM- Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b37604",
   "metadata": {},
   "source": [
    "***feature selection***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "470e8868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features  (0, 24626)\t2\n",
      "  (0, 40333)\t1\n",
      "  (0, 51674)\t1\n",
      "  (0, 36323)\t1\n",
      "  (0, 16586)\t2\n",
      "  (0, 47165)\t1\n",
      "  (0, 23395)\t1\n",
      "  (0, 16653)\t5\n",
      "  (0, 16510)\t1\n",
      "  (0, 46552)\t2\n",
      "  (0, 15057)\t3\n",
      "  (0, 50846)\t1\n",
      "  (0, 38736)\t1\n",
      "  (0, 18505)\t1\n",
      "  (0, 51186)\t1\n",
      "  (0, 46564)\t1\n",
      "  (0, 35505)\t1\n",
      "  (0, 8363)\t2\n",
      "  (0, 16985)\t1\n",
      "  (0, 15829)\t1\n",
      "  (0, 44750)\t1\n",
      "  (0, 18546)\t1\n",
      "  (0, 39272)\t2\n",
      "  (0, 46443)\t4\n",
      "  (0, 7660)\t2\n",
      "  :\t:\n",
      "  (24999, 26663)\t1\n",
      "  (24999, 26521)\t1\n",
      "  (24999, 51393)\t1\n",
      "  (24999, 34140)\t1\n",
      "  (24999, 10321)\t1\n",
      "  (24999, 42491)\t1\n",
      "  (24999, 10047)\t1\n",
      "  (24999, 41103)\t1\n",
      "  (24999, 44398)\t1\n",
      "  (24999, 43586)\t1\n",
      "  (24999, 43667)\t1\n",
      "  (24999, 31963)\t1\n",
      "  (24999, 3249)\t2\n",
      "  (24999, 40649)\t1\n",
      "  (24999, 9868)\t1\n",
      "  (24999, 25326)\t1\n",
      "  (24999, 4387)\t1\n",
      "  (24999, 37972)\t1\n",
      "  (24999, 13063)\t1\n",
      "  (24999, 34830)\t1\n",
      "  (24999, 43807)\t1\n",
      "  (24999, 42567)\t1\n",
      "  (24999, 44744)\t1\n",
      "  (24999, 5450)\t1\n",
      "  (24999, 43297)\t1\n",
      "Testing Features  (0, 402)\t1\n",
      "  (0, 882)\t1\n",
      "  (0, 1161)\t1\n",
      "  (0, 1331)\t1\n",
      "  (0, 1503)\t1\n",
      "  (0, 1518)\t1\n",
      "  (0, 2238)\t1\n",
      "  (0, 2627)\t1\n",
      "  (0, 3306)\t1\n",
      "  (0, 4548)\t1\n",
      "  (0, 4788)\t1\n",
      "  (0, 4807)\t1\n",
      "  (0, 5221)\t3\n",
      "  (0, 5560)\t2\n",
      "  (0, 5722)\t2\n",
      "  (0, 6307)\t1\n",
      "  (0, 6863)\t3\n",
      "  (0, 6869)\t2\n",
      "  (0, 7303)\t1\n",
      "  (0, 7596)\t1\n",
      "  (0, 7641)\t1\n",
      "  (0, 7861)\t1\n",
      "  (0, 8529)\t2\n",
      "  (0, 8697)\t1\n",
      "  (0, 8861)\t1\n",
      "  :\t:\n",
      "  (24999, 29106)\t1\n",
      "  (24999, 29450)\t1\n",
      "  (24999, 30736)\t1\n",
      "  (24999, 31154)\t1\n",
      "  (24999, 32253)\t1\n",
      "  (24999, 33340)\t1\n",
      "  (24999, 34953)\t1\n",
      "  (24999, 37898)\t1\n",
      "  (24999, 38023)\t1\n",
      "  (24999, 41138)\t1\n",
      "  (24999, 41386)\t1\n",
      "  (24999, 42394)\t1\n",
      "  (24999, 44750)\t1\n",
      "  (24999, 45108)\t1\n",
      "  (24999, 45125)\t1\n",
      "  (24999, 45944)\t1\n",
      "  (24999, 46016)\t1\n",
      "  (24999, 46443)\t1\n",
      "  (24999, 46472)\t1\n",
      "  (24999, 46552)\t1\n",
      "  (24999, 47338)\t1\n",
      "  (24999, 47731)\t1\n",
      "  (24999, 48623)\t1\n",
      "  (24999, 50349)\t3\n",
      "  (24999, 50619)\t1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Transform each text into a vector of word counts\n",
    "vectorizer = CountVectorizer(stop_words=\"english\",\n",
    "                             preprocessor=clean_text\n",
    "                             )\n",
    "training_features = vectorizer.fit_transform(train_data[\"text\"]) \n",
    "print('Training Features' + str(training_features))\n",
    "test_features = vectorizer.transform(test_data[\"text\"])\n",
    "print('Testing Features' + str(test_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5596c9",
   "metadata": {},
   "source": [
    "###  \n",
    "****Training****\n",
    "* After performing feature selection, we train a Linear Support Vector Machine (SVM) model using the selected features. SVMs are known for their ability to handle high-dimensional data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c4603b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Training\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(training_features, train_data[\"sentiment\"])\n",
    "y_pred = model.predict(test_features)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e70cb",
   "metadata": {},
   "source": [
    "###  \n",
    "***Evaluation***\n",
    "* Because the IMDB dataset is balanced, we can evaluate our model using the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3afcb71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the IMDB dataset: 82.60\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
    "print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb2c59",
   "metadata": {},
   "source": [
    "* As you can see, we were able to reach as high as an 83.67% accuracy on the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4c0d9",
   "metadata": {},
   "source": [
    "## KNN- Classification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c3d5ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:   (0, 24626)\t2\n",
      "  (0, 40333)\t1\n",
      "  (0, 51674)\t1\n",
      "  (0, 36323)\t1\n",
      "  (0, 16586)\t2\n",
      "  (0, 47165)\t1\n",
      "  (0, 23395)\t1\n",
      "  (0, 16653)\t5\n",
      "  (0, 16510)\t1\n",
      "  (0, 46552)\t2\n",
      "  (0, 15057)\t3\n",
      "  (0, 50846)\t1\n",
      "  (0, 38736)\t1\n",
      "  (0, 18505)\t1\n",
      "  (0, 51186)\t1\n",
      "  (0, 46564)\t1\n",
      "  (0, 35505)\t1\n",
      "  (0, 8363)\t2\n",
      "  (0, 16985)\t1\n",
      "  (0, 15829)\t1\n",
      "  (0, 44750)\t1\n",
      "  (0, 18546)\t1\n",
      "  (0, 39272)\t2\n",
      "  (0, 46443)\t4\n",
      "  (0, 7660)\t2\n",
      "  :\t:\n",
      "  (24999, 26663)\t1\n",
      "  (24999, 26521)\t1\n",
      "  (24999, 51393)\t1\n",
      "  (24999, 34140)\t1\n",
      "  (24999, 10321)\t1\n",
      "  (24999, 42491)\t1\n",
      "  (24999, 10047)\t1\n",
      "  (24999, 41103)\t1\n",
      "  (24999, 44398)\t1\n",
      "  (24999, 43586)\t1\n",
      "  (24999, 43667)\t1\n",
      "  (24999, 31963)\t1\n",
      "  (24999, 3249)\t2\n",
      "  (24999, 40649)\t1\n",
      "  (24999, 9868)\t1\n",
      "  (24999, 25326)\t1\n",
      "  (24999, 4387)\t1\n",
      "  (24999, 37972)\t1\n",
      "  (24999, 13063)\t1\n",
      "  (24999, 34830)\t1\n",
      "  (24999, 43807)\t1\n",
      "  (24999, 42567)\t1\n",
      "  (24999, 44744)\t1\n",
      "  (24999, 5450)\t1\n",
      "  (24999, 43297)\t1\n",
      "Testing Features:   (0, 402)\t1\n",
      "  (0, 882)\t1\n",
      "  (0, 1161)\t1\n",
      "  (0, 1331)\t1\n",
      "  (0, 1503)\t1\n",
      "  (0, 1518)\t1\n",
      "  (0, 2238)\t1\n",
      "  (0, 2627)\t1\n",
      "  (0, 3306)\t1\n",
      "  (0, 4548)\t1\n",
      "  (0, 4788)\t1\n",
      "  (0, 4807)\t1\n",
      "  (0, 5221)\t3\n",
      "  (0, 5560)\t2\n",
      "  (0, 5722)\t2\n",
      "  (0, 6307)\t1\n",
      "  (0, 6863)\t3\n",
      "  (0, 6869)\t2\n",
      "  (0, 7303)\t1\n",
      "  (0, 7596)\t1\n",
      "  (0, 7641)\t1\n",
      "  (0, 7861)\t1\n",
      "  (0, 8529)\t2\n",
      "  (0, 8697)\t1\n",
      "  (0, 8861)\t1\n",
      "  :\t:\n",
      "  (24999, 29106)\t1\n",
      "  (24999, 29450)\t1\n",
      "  (24999, 30736)\t1\n",
      "  (24999, 31154)\t1\n",
      "  (24999, 32253)\t1\n",
      "  (24999, 33340)\t1\n",
      "  (24999, 34953)\t1\n",
      "  (24999, 37898)\t1\n",
      "  (24999, 38023)\t1\n",
      "  (24999, 41138)\t1\n",
      "  (24999, 41386)\t1\n",
      "  (24999, 42394)\t1\n",
      "  (24999, 44750)\t1\n",
      "  (24999, 45108)\t1\n",
      "  (24999, 45125)\t1\n",
      "  (24999, 45944)\t1\n",
      "  (24999, 46016)\t1\n",
      "  (24999, 46443)\t1\n",
      "  (24999, 46472)\t1\n",
      "  (24999, 46552)\t1\n",
      "  (24999, 47338)\t1\n",
      "  (24999, 47731)\t1\n",
      "  (24999, 48623)\t1\n",
      "  (24999, 50349)\t3\n",
      "  (24999, 50619)\t1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Transform each text into a vector of word counts\n",
    "vectorizer = CountVectorizer(stop_words=\"english\",\n",
    "                             preprocessor=clean_text\n",
    "                             )\n",
    "training_features = vectorizer.fit_transform(train_data[\"text\"]) \n",
    "print('Training Features:', training_features)\n",
    "test_features = vectorizer.transform(test_data[\"text\"])\n",
    "print('Testing Features:', test_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7ca0511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiments: [1 1 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(training_features, train_data[\"sentiment\"])\n",
    "y_pred = model.predict(test_features)\n",
    "print('Predicted Sentiments:', y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9230a903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the IMDB dataset: 63.48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
    "print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333ca80",
   "metadata": {},
   "source": [
    "* with this model we were able to reach as high as an 63.48% accuracy on the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ef6ed",
   "metadata": {},
   "source": [
    "## Improving the Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96fa26c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features  (0, 645242)\t0.06732247365603838\n",
      "  (0, 422494)\t0.07004452730214439\n",
      "  (0, 944148)\t0.06732247365603838\n",
      "  (0, 405678)\t0.0616342154456931\n",
      "  (0, 191199)\t0.04677762008373133\n",
      "  (0, 217168)\t0.06539114585846177\n",
      "  (0, 666343)\t0.07004452730214439\n",
      "  (0, 550428)\t0.06732247365603838\n",
      "  (0, 198811)\t0.07004452730214439\n",
      "  (0, 789336)\t0.07004452730214439\n",
      "  (0, 1341147)\t0.03957933010875477\n",
      "  (0, 1416275)\t0.06539114585846177\n",
      "  (0, 2295)\t0.07004452730214439\n",
      "  (0, 993186)\t0.04637062212124525\n",
      "  (0, 1073266)\t0.05102400356492786\n",
      "  (0, 956672)\t0.07004452730214439\n",
      "  (0, 488849)\t0.07004452730214439\n",
      "  (0, 779648)\t0.0616342154456931\n",
      "  (0, 1341127)\t0.03972449058206764\n",
      "  (0, 822689)\t0.02840436127543072\n",
      "  (0, 1466467)\t0.041968708664902175\n",
      "  (0, 797951)\t0.04709992567185625\n",
      "  (0, 565694)\t0.053648049480162684\n",
      "  (0, 1165650)\t0.07004452730214439\n",
      "  (0, 628829)\t0.07004452730214439\n",
      "  :\t:\n",
      "  (24999, 1205922)\t0.05677013565919056\n",
      "  (24999, 635911)\t0.045113965229896295\n",
      "  (24999, 26469)\t0.037325801907366146\n",
      "  (24999, 68637)\t0.033403957479621195\n",
      "  (24999, 163172)\t0.10169772171710156\n",
      "  (24999, 266074)\t0.06877919514918694\n",
      "  (24999, 542659)\t0.07438802391471744\n",
      "  (24999, 468217)\t0.04056083880461339\n",
      "  (24999, 1208054)\t0.04926196834821318\n",
      "  (24999, 1288530)\t0.08736971016477252\n",
      "  (24999, 266025)\t0.04862155156219569\n",
      "  (24999, 540369)\t0.08918559495916434\n",
      "  (24999, 1260320)\t0.034362355694598806\n",
      "  (24999, 505920)\t0.03710584473112933\n",
      "  (24999, 162153)\t0.04202612863391723\n",
      "  (24999, 915407)\t0.04076326283261414\n",
      "  (24999, 784823)\t0.03623788894904002\n",
      "  (24999, 1269114)\t0.06296138751887989\n",
      "  (24999, 539858)\t0.03501776673782312\n",
      "  (24999, 593178)\t0.021703930217256775\n",
      "  (24999, 1506709)\t0.043006736447731786\n",
      "  (24999, 578992)\t0.02696143362764785\n",
      "  (24999, 625053)\t0.06314139258054935\n",
      "  (24999, 1338164)\t0.012519295612182233\n",
      "  (24999, 498755)\t0.017259802164148526\n",
      "Testing Features  (0, 1516595)\t0.06516054106711107\n",
      "  (0, 1516129)\t0.02402457425429888\n",
      "  (0, 1511568)\t0.06516054106711107\n",
      "  (0, 1511197)\t0.029337437858932832\n",
      "  (0, 1503418)\t0.03930996907511817\n",
      "  (0, 1500443)\t0.07788653039638159\n",
      "  (0, 1499187)\t0.02338830889609454\n",
      "  (0, 1498565)\t0.042530684456448906\n",
      "  (0, 1497146)\t0.07104639599210791\n",
      "  (0, 1496566)\t0.06587203406521618\n",
      "  (0, 1496354)\t0.05747514299102253\n",
      "  (0, 1473432)\t0.03721042160601198\n",
      "  (0, 1466514)\t0.05998617914021935\n",
      "  (0, 1466274)\t0.06968536077709894\n",
      "  (0, 1464982)\t0.0350264603046591\n",
      "  (0, 1459549)\t0.056105295168244924\n",
      "  (0, 1459226)\t0.026520447223716295\n",
      "  (0, 1443597)\t0.0490660815983496\n",
      "  (0, 1434645)\t0.07788653039638159\n",
      "  (0, 1433783)\t0.026821670705703533\n",
      "  (0, 1426909)\t0.05034894828454102\n",
      "  (0, 1426740)\t0.015372562473936778\n",
      "  (0, 1419189)\t0.02753316370380865\n",
      "  (0, 1404007)\t0.07788653039638159\n",
      "  (0, 1403893)\t0.04446309335954418\n",
      "  :\t:\n",
      "  (24999, 306069)\t0.13684331815141168\n",
      "  (24999, 305239)\t0.05404206861532328\n",
      "  (24999, 289548)\t0.060428052224718545\n",
      "  (24999, 262278)\t0.14088498619054488\n",
      "  (24999, 261823)\t0.03593522036775891\n",
      "  (24999, 257915)\t0.1465813973518595\n",
      "  (24999, 257816)\t0.06900995909413767\n",
      "  (24999, 222785)\t0.12710523895096387\n",
      "  (24999, 222132)\t0.02957772785261314\n",
      "  (24999, 135479)\t0.10558940708677322\n",
      "  (24999, 134989)\t0.04260091780138225\n",
      "  (24999, 131857)\t0.08519468116385277\n",
      "  (24999, 130477)\t0.04127203767556672\n",
      "  (24999, 126390)\t0.12397027766610146\n",
      "  (24999, 125355)\t0.03352579892306945\n",
      "  (24999, 110054)\t0.12140882778964925\n",
      "  (24999, 109124)\t0.03431402038834744\n",
      "  (24999, 91064)\t0.09134287011311386\n",
      "  (24999, 89940)\t0.06274453294902164\n",
      "  (24999, 73751)\t0.10222845038874606\n",
      "  (24999, 73346)\t0.04607662419482498\n",
      "  (24999, 68379)\t0.1465813973518595\n",
      "  (24999, 67754)\t0.057920093601351336\n",
      "  (24999, 25433)\t0.12140882778964925\n",
      "  (24999, 24274)\t0.032390556194553326\n",
      "[1 1 1 ... 0 0 0]\n",
      "Accuracy on the IMDB dataset: 88.75\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Transform each text into a vector of word counts\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "                             preprocessor=clean_text,\n",
    "                             ngram_range=(1, 2))\n",
    "training_features = vectorizer.fit_transform(train_data[\"text\"]) \n",
    "print('Training Features' + str(training_features))\n",
    "test_features = vectorizer.transform(test_data[\"text\"])\n",
    "print('Testing Features' + str(test_features))\n",
    "\n",
    "\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(training_features, train_data[\"sentiment\"])\n",
    "y_pred = model.predict(test_features)\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
    "print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbea4a5",
   "metadata": {},
   "source": [
    "* We achieve an even higher accuracy score of 88.66% which is another 2% improvement over the last version of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
